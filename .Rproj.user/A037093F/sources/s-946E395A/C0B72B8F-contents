---
title: "Pooling Loss (Source Bottles) ML Model & Analytics"
author: "Created by: Alexis Roldan"
date: "Updated On: `r format(Sys.Date(),'%d%b%y')`"
always_allow_html: yes
output:
  rmdformats::readthedown:
    highlight: kate
  html_document: 
    fig_height: 6.5
    fig_width: 9
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: console
---


<!-- To set up all chunks to cache = true as default -->
```{r setup, include=FALSE}
library(knitr)
library(rgl)
knit_hooks$set(webgl = hook_webgl)
knitr::opts_chunk$set(cache=FALSE, message = FALSE, warning = FALSE,
                      fig.align = "center")
```


<!-- Ctr+Shift+C for comments, need to highlight the lines of code -->

```{css style-customizations,echo=FALSE}
h1.title {
  font-size: 33px;
  color: Navy;
  font-family:"Roboto"
}

h1 {
  font-size: 27px;
  color: blue;
  font-family:"Helvetica"
  
}
h2 {
  font-size: 24px;
  color: brown;
  font-family:"Helvetica"
}

h3 {
  font-size: 21px;
  color: Navy;
  font-family:"Helvetica"
}

h4 {
  font-size: 18px;
  color: black;
  font-family:"Helvetica"
}
```


**Before we begin please note anything bolded is either a finding or an important comment**


# Source Bottle Pooling Loss - Business Understanding

During __"Pooling Loss Weekly Meeting"__ led by Manufacturing Science, the team while analyzing __"Teardown Process & Yield Performance BI Dashboard"__ trends and by collecting operator's feedback, have observed a decrease YTD on __Source Bottles Pooling Loss__ while average still above target due to different factors within _Rollerthaw Process & Cryo Centrifugation_.

Based on that premise, __Source Bottle Pooling Loss__ analysis needs to be performed to understand pooling loss increase.


# Source Bottle Pooling Loss - Data Understanding

After having an understanding on what is our business problem, __initial dataset__ is loaded for data preparation and exploration prior model development.


## Libraries & Functions

Let's begin by loading the libraries and functions that will be used for this analysis:

```{r Libraries, echo=FALSE}

load.libraries <- c('plyr', 'dplyr','data.table', 'readxl',
                    'stringr', 'stringi','forecast',
                    'tidyverse','matrixStats','lubridate','e1071','xgboost',
                    'caret','zoo','plotly','DT','rpart','tidyr','inspectdf','DataExplorer',
                    'gridExtra','changepoint','ggfortify','magrittr','ggpmisc','prophet',
                    'ggplot2','tsoutliers','dlm','reshape2','AppliedPredictiveModeling',
                    'PerformanceAnalytics','NbClust','fpc','flexclust',
                    'ggdendro','rgdal','tsibble')

install.lib <- load.libraries[!load.libraries %in% installed.packages()]
for(libs in install.lib) install.packages(libs, dependences = TRUE)
sapply(load.libraries, require, character = TRUE)

library("corrplot")
library("Hmisc")
library("RColorBrewer")
library("dendextend")
library("feasts")
library("factoextra")
library(Boruta)
library(bounceR)
library(randomForest)
library(mlbench)
library(randomForestExplainer)
library(pdp) 
library(gbm)


#Loading all the plotting functions
plotHist <- function(data_in, i) {
  data <- data.frame(x=data_in[[i]])
  p <- ggplot(data=data, aes(x=x)) + geom_histogram(bins=20, fill="#0072B2", alpha = .9) +
    xlab(colnames(data_in)[i]) + theme_light() + 
    theme(axis.text.x = element_text(angle = 90, hjust =1))
  return (p)
}

plotCorr <- function(data_in, list1,list2,i){
  data <- data.frame(x = data_in[[list1[i]]], y = data_in[[list2[i]]])
  p <- ggplot(data, aes(x = x, y = y)) + geom_smooth(method = lm ) + 
    geom_point(aes(x = x, y = y)) +
    geom_jitter(width = 0.1, height = 0.1)  + 
    xlab(paste0(list1[i], '\n', 'R-Squared: ', 
                round(cor(data_in[[list1[i]]], 
                          data_in[[list2[i]]], 
                          use = 'pairwise.complete.obs'), 3))) + 
    theme_light() + ylab(paste0(list2[i]))
  return(suppressWarnings(p))
}

doPlotsCorr <- function(data_in, fun, list1,list2,ii, ncol=3) {
  pp <- list()
  for (i in ii) {
    p <- fun(data_in=data_in, list1,list2,i=i)
    pp <- c(pp, list(p))
  }
  do.call("grid.arrange", c(pp, ncol=ncol))
}

doPlots <- function(data_in, fun, ii, ncol=3) {
  pp <- list()
  for (i in ii) {
    p <- fun(data_in=data_in, i=i)
    pp <- c(pp, list(p))
  }
  do.call("grid.arrange", c(pp, ncol=ncol))
}

```


## Data Collection

Raw data for each continuous parameter are loaded.

<!-- ALt+Ctrl+I is the shorcut to insert R code -->
```{r read Pooling Loss Data}

#load raw data
## Load data
load("rda/Source_Bottles_Raw.rda")

names(Source_Bottles_Raw) <- c("Batchid", "TD_Start", "Cryo_Vessel",
                               "FrI_Vessel", "Recipe_Load", "Bin_Performance",
                               "Final_Transfer","Wait_Saline_Start", "Saline_Flush",
                               "Wait_Cryo_Cent_Start", "Cryo_Cent", "Voucher_Plasma",
                               "Cryo_Poor_Plasma", "Cryo_Rich_Plasma", "Vessel_Weight_Before_Saline",
                               "Saline_Weight", "Vessel_Weight_After_Saline", "FrI_Vessel_Weight",
                               "Cryo_Paste_Weight", "APDS_Discard", "Pooling_Loss",
                               "Actual_Saline_in_Vessel", "Plasma_loss_Bin_Process",
                               "Pooling_Result")

RT_Predictors <- Source_Bottles_Raw[,-c(1:4, 21, 24)]

```

**Data Summary**

The 2020 YTD Rollerthaw dataset (Source_Bottles_Raw) has `r dim(Source_Bottles_Raw)[1]` rows and `r dim(Source_Bottles_Raw)[2]` columns  
  

## Exploratory Data Analysis (EDA)

After raw data is loaded, I proceed to perform EDA on dataset.

### Data Cleaning {.tabset .tabset-fade .tabset-pills}

For this step, I perform a data cleaning if needed.

```{r Data cleaning}

introduce(Source_Bottles_Raw)

```

#### Percentages
``` {r Percentages, message=FALSE, warning=FALSE, echo=FALSE, dev='png'}
# Explore Data
plot_intro(Source_Bottles_Raw)
```

#### Inspection Categorical Level
``` {r Structure Data, message=FALSE, warning=FALSE, echo=FALSE, out.width="110%"}
## Inspect Categorical Data on Pooling Loss
show_plot(inspect_cat(Source_Bottles_Raw))

```

#### Missing Data
``` {r Missing Data, message=FALSE, warning=FALSE, echo=FALSE, dev='png'}
# Explore Data
plot_missing(Source_Bottles_Raw) ## Are there missing values, and what is the missing data profile?

```


### Time-series Analysis {.tabset .tabset-fade .tabset-pills}

For this analysis, I visualize _Rollerthaw Pooling Loss Performance_ over time:

```{r Time Series, echo=FALSE}

# Create a ts object for Source
RT.value.ts <- ts(Source_Bottles_Raw$Pooling_Loss,
                  frequency = 1)
#plot(RT.value.ts)

mvalue.s <- cpt.mean(RT.value.ts,
                     method = "BinSeg")
#cpts(mvalue.s)

```

#### Change in Mean

Here I perform a *mean changepoint analysis* to identify significant changes in time series:

```{r Bioscience Lots 1, dev='png',  echo=FALSE, warning=FALSE}

plot(mvalue.s, pt.width=10, cpt.col='blue' , 
     xlab='Lots', ylab='Minutes', main = "Rollerthaw Pooling Loss Performance (01Jan20 - 01Mar20)
     Changes in Mean (BinSeg)",
          col = "brown")
legend("topleft", legend = c("Actual", "Mean"),
       bty = "n",
       col=c("brown", "blue"), lwd = 1, cex=0.8)

mvalue.s


# print("Mean Changepoints Start Lot & Date/Time:")

# knitr::kable(rbind(Source_Bottles_Raw[1,c(1,2,5,3)])

```

#### Breakpoint, Smoothed & Spectral Analysis

After changes have been identified in time series analysis above, I proceed to perform *breakdown, smoothed & spectral density analysis* to help understand time series behaviour:

```{r Bioscience Lots 2, dev='png', echo=FALSE}

# Identify change points in mean and variance on Source
p1 <- RT.value.ts %>%
  cpt.mean(method = "BinSeg") %>%  # Identify change points
  autoplot(pt.width=10, cpt.col='blue', col="brown",
           xlab="Lots", ylab = "Minutes", main = "Rollerthaw Pooling Loss - Change in Mean",
           cpt.linetype = 6) + 
  theme_classic()

# Detect jump in a data
p2 <- strucchange::breakpoints(RT.value.ts ~ 1) %>%
  autoplot(pt.width=10, cpt.col='blue', col="brown",
           xlab="Lots", ylab = "Minutes", main = "Rollerthaw Pooling Loss Performance - Change in Breakpoints",
           cpt.linetype = 6) + 
  theme_classic()

# Create a smooth time series for source timeseries
form <- function(theta){
  dlmModPoly(order = 1, dV = exp(theta[1]), dW = exp(theta[2]))
}

source_model <- form(dlmMLE(RT.value.ts, parm = c(1, 1), form)$par)
source_filtered <- dlmFilter(RT.value.ts, source_model)
p3 <- autoplot(source_filtered, ts.linetype = 'dashed', fitted.colour = 'blue') +
  labs(title = "Rollerthaw Pooling Loss Performance - Smoothed VS Actual") +
  xlab("Lots") + ylab("Minutes") +
  theme_classic()

# Create spectral density analysis for source lots
p4 <- autoplot(spec.ar(RT.value.ts, plot = FALSE)) + 
  theme_classic() + 
  labs(title = "Rollerthaw Pooling Loss Performance - Spectral Density Analysis")

grid.arrange(p1, p2, p3, p4, nrow = 2)

```

#### Trend, Seasonality & ARIMA Model

Here I first display scatter lag plots for *Rollerthaw Pooling Loss Performance*, where the horizontal axis shows lagged values of the time series. 

Each graph shows $y_{t}$ plotted against $y_{t-k}$ for different values of $k$.

```{r Bioscience Lots 3, dev='png', echo=FALSE}

RT_PL_TS <- data.frame(Source_Bottles_Raw[,c(2)], Source_Bottles_Raw[,c(24)],
                       Source_Bottles_Raw[,c(21)])


RT_PL_TS <- RT_PL_TS %>%
  as_tsibble()

RT_PL_TS %>% 
  gg_lag(Pooling_Loss, geom="point",
         col = "navy") 

```

Additionally, __Decomposition Analysis__ is performed to determine trend or seasonality. 

_Autocorrelation & Partial Autocorrelation Analysis_ are performed to measure the linear relationship between lagged values of this time series.

The value for each *autocorrelation coefficient* $r_{k}$ can be expressed as:

$r_{k} = \frac{\sum\limits_{t=k+1}^T (y_{t}-\bar{y})(y_{t-k}-\bar{y})} {\sum\limits_{t=1}^T (y_{t}-\bar{y})^2}$

where $T$ is the length of the time series:

```{r Bioscience Lots 4, dev='png',  echo=FALSE}

ggtsdisplay(RT.value.ts, 
            plot.type = "partial",
            main = "ACF & PACF plot for `Rollerthaw Pooling Loss' Time-Series",
            smooth = TRUE,
            theme = theme_classic())


```

Based on *autocorrelation and partial autocorrelation plots*, time-series **Do Not** show any autocorrelation (white noise); therefore, I assume that time series is already a *stationary series*. No need to transform it any further.

```{r Bioscience Lots 5, dev='png',  echo=FALSE}

print("We now test the time-series to verify if is already a stationary series:")
tseries::adf.test(RT.value.ts, 
                  alternative="stationary", k=0)

print("Now I proceed to create an ARIMA model (model with lowest AIC below):")

auto.arima(RT.value.ts)

```

**Plot Time Series Diagnostics**

Here I proceed to evaluate *ARIMA* model created by plotting residuals. As we can see from *Ljung-Box test* results below, *p-value > 0.05*, therefore, we observe that there's not enough statistical evidence to reject the null hypothesis. In other words, we cannot assume that residual values are dependent which favors ARIMA model:

```{r Bioscience Lots 6, dev='png',  echo=FALSE}

# ggtsdiag(auto.arima(RT.value.ts)) + theme_bw()

checkresiduals(auto.arima(RT.value.ts),
               theme = theme_classic())

# Outliers Source lots
data.ts.outliers <- tso(RT.value.ts)

```

**Outlier Analysis**

Here I proceed to point the `r data.ts.outliers$cval` outliers found during ARIMA model and plot effect of outliers in trend.

```{r Bioscience Lots 7, dev='png', echo=FALSE}

data.ts.outliers
#plot(data.ts.outliers)

```

### Forecast

This analysis will create a _Rollerthaw Pooling Loss_ forecast for the next 45 lots:

```{r Forecast, dev='png', echo=FALSE, out.width="105%"}

# Create dataset
RT_Forecast_Source <- data.frame(Source_Bottles_Raw[,c(2)],
                               Source_Bottles_Raw[,c(21)])

names(RT_Forecast_Source)[1] <- "ds"
names(RT_Forecast_Source)[2] <- "y"

# Create Forecast
Forecast_RT <- prophet(RT_Forecast_Source)
future_RT <- make_future_dataframe(Forecast_RT, periods = 25)
#tail(future_RT)

Forecast_RT_Pred <- predict(Forecast_RT, future_RT)
#tail(Forecast_RT_Pred[c('ds', 'yhat', 'yhat_lower', 'yhat_upper')])

# Plot Forecast
# plot(Forecast_RT, Forecast_RT_Pred) + 
#   labs(title = "FP006 Source Filtration Forecast") + 
#   xlab("Date") + ylab("Minutes") +
#   theme_classic()

dyplot.prophet(Forecast_RT, Forecast_RT_Pred,
               main = "Rollerthaw Pooling Loss Forecast")

```

Please see below for __Rollerthaw Time Analysis Component Breakdown & Plot Prediction__:

```{r Forecast Breakdown, dev='png', echo=FALSE}

prophet_plot_components(Forecast_RT, Forecast_RT_Pred)

# Model Performance
# get prediction only for the amount of original datapoints
pred_RT <- Forecast_RT_Pred$yhat[1:44]
actual_RT <- Forecast_RT$history$y

# Plot Prediction
plot(actual_RT, pred_RT, 
     main = "FP006 Source Filtration Actual vs Predicted",
     xlab = "Actual (min)", ylab = "Predicted (min)", col = c("blue", "brown"))
abline(lm(pred_RT~actual_RT), col = 'red')
legend("topleft", legend = c("Actual", "Predicted"),
       bty = "n",
       col=c("blue", "brown"), pch = 1, cex=0.8)

# Cross validation
#x_FP006_source <- cross_validation(Forecast_RT, 90, units = 'days')
#performance_metrics(x_FP006_source, rolling_window = 0.1)

#plot_cross_validation_metric(x_FP006_source, metric = 'rmse',
#                             rolling_window = 0.1) +
#  labs(title = "FP006 Source Filtration - Performance VS Forecast (Cross Validation)") +
#  theme_classic()

```

As we can see above, component breakdown analysis show an upper-trend after **January 2020** and from **Monday to Tuesday and Friday to Saturday with emphasys after noon, after 6pm and after midnight**.


### Feature Analytics

Here I proceed to analyze features in dataset:

```{r Feature Exploration, dev='png', echo=FALSE}

# Reshaping dataset
Source_Bottles_Raw_Melt <- melt(Source_Bottles_Raw[,-c(1,2,4)])

```

#### Histogram & Boxplot {.tabset .tabset-fade .tabset-pills}

##### Feature Histogram 1

```{r Histogram 1, dev='png', echo=FALSE}

doPlots(Source_Bottles_Raw[,-c(1:4, 24)], plotHist, ii = 1:9)

```

##### Feature Histogram 2

```{r Histogram 2, dev='png', echo=FALSE}

doPlots(Source_Bottles_Raw[,-c(1:4, 21, 24)], plotHist, ii = 10:18)

```

##### Histogram by Pooling Result

```{r Histogram 3, dev='png', echo=FALSE}

# Inspect data by Cryo Sending Vessel
library(AppliedPredictiveModeling)
library(leaps)      # model selection functions

## Plot Features after feature engineer output variable
transparentTheme(trans = .9)
featurePlot(x = Source_Bottles_Raw[,-c(1:4, 21, 24)], 
            y = Source_Bottles_Raw$Pooling_Result,
            plot = "density", 
            ## Pass in options to xyplot() to 
            ## make it prettier
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")), 
            adjust = 1.5, 
            pch = "|", 
            layout = c(3, 3), 
            auto.key = list(columns = 3))

```

##### Boxplot by Pooling Result

```{r Boxplot 4, dev='png', echo=FALSE}

transparentTheme(trans = .9)
featurePlot(x = Source_Bottles_Raw[,-c(1:4, 21, 24)], 
            y = Source_Bottles_Raw$Pooling_Result,
            plot = "box", 
            ## Pass in options to bwplot() 
            scales = list(y = list(relation="free"),
                          x = list(rot = 90)),  
            layout = c(3,3), 
            auto.key = list(columns = 3))


```

#### Summary Table by Cryo Vessel {.tabset .tabset-fade .tabset-pills}


```{r Summary Aggregated, message=FALSE, warning=FALSE, dev='png', echo=FALSE}

# function to calculate mode
fun.mode<-function(x){as.numeric(names(sort(-table(x)))[1])}

test1 <- Source_Bottles_Raw_Melt %>% filter(Cryo_Vessel == "V-004") %>% 
  group_by(variable) %>%
  summarise(Min = round(min(value, na.rm = TRUE),2),
            Median = round(median(value, na.rm = TRUE),2),
            Mean = round(mean(value, na.rm = TRUE),2),
            Mode = round(fun.mode(value),2),
            Max = round(max(value, na.rm = TRUE),2),
            Standard_Deviation = round(sd(value, na.rm = TRUE),2),
            P25 = round(quantile(value, na.rm = TRUE,.25),2),
            P75 = round(quantile(value, na.rm = TRUE,.75),2),
            Kurtosis = round(psych::kurtosi(value, na.rm = TRUE),2)
  )

test2 <- Source_Bottles_Raw_Melt %>% filter(Cryo_Vessel == "V-005") %>% 
  group_by(variable) %>%
  summarise(Min = round(min(value, na.rm = TRUE),2),
            Median = round(median(value, na.rm = TRUE),2),
            Mean = round(mean(value, na.rm = TRUE),2),
            Mode = round(fun.mode(value),2),
            Max = round(max(value, na.rm = TRUE),2),
            Standard_Deviation = round(sd(value, na.rm = TRUE),2),
            P25 = round(quantile(value, na.rm = TRUE,.25),2),
            P75 = round(quantile(value, na.rm = TRUE,.75),2),
            Kurtosis = round(psych::kurtosi(value, na.rm = TRUE),2)
  )

# DT::datatable(test1)
# DT::datatable(test2)

```

##### Cryo Vessel V-004

```{r Summary V004, echo=FALSE}

knitr::kable(test1)

```

##### Cryo Vessel V-005

```{r Summary V005, echo=FALSE}

knitr::kable(test2)

```

### QQ Plot

```{r QQ Plot, message=FALSE, warning=FALSE, dev='png', echo=FALSE}

# QQ Plot
plot_qq(Source_Bottles_Raw[,-c(1:4, 21)], by = "Pooling_Result",
        nrow = 3L, ncol = 3L,
        title = "Rollerthaw Features - QQ Plot",
        ggtheme = theme_light()) 
```

### Principal Component Analysis

```{r PCA, message=FALSE, warning=FALSE, dev='png', echo=FALSE}

# PCA
PCA_RT_Feat <- na.omit(RT_Predictors)
plot_prcomp(PCA_RT_Feat, variance_cap = 0.9, 
            nrow = 3L, ncol = 3L,
            title = "Rollerthaw Features - PCA",
            ggtheme = theme_light())


```

## Correlated Features

Here I do a first pass on feature selection based on correlation analysis between features.

### Between Predictor-Correlations

```{r Correlation 1, warning=FALSE, dev='png', echo=FALSE}

## Correlation at glance
RT_Pred_Corr <- RT_Predictors
colnames(RT_Pred_Corr)[1:ncol(RT_Pred_Corr)] <- c(1:ncol(RT_Pred_Corr))
RT_corr <- cor(na.omit(RT_Pred_Corr))
corrplot(RT_corr, method="square",type = "upper")

```

**A quick glance, we can see that there are some predictors that are highly correlated**

```{r, echo=FALSE}

## Evaluate Predicto Correlation
RT_df_corr <- cor(RT_Predictors, use = "pairwise.complete.obs")
hc_RT_Pred <- findCorrelation(RT_df_corr, cutoff = 0.7)
hc_RT_Pred <- sort(hc_RT_Pred)
RT_Pred_df2 <- as.data.frame(RT_Predictors)[,-c(hc_RT_Pred)]

RT_Remove_Col_HC <- setdiff(colnames(RT_Predictors),
                            colnames(RT_Pred_df2))

```

There are `r length(RT_Remove_Col_HC)` columns that have been identified with high correlation with a cutoff set at 0.70

Below is the table showing the variables that have a correlation abs > 0.7

```{r, echo=FALSE}

#Highly correlated vairables table format
RT_df_corr_2 <- RT_df_corr %>%
  as.data.frame() %>%
  mutate(var1 = rownames(.)) %>%
  gather(var2, value, -var1) %>%
  arrange(desc(value)) %>%
  group_by(value)

RT_corr_tb <- setDT(RT_df_corr_2)[abs(value) > 0.7 &
                                 var1 != var2 & 
                                 var1 != "Pooling_Loss"  & var2 != "Pooling_Loss"]

RT_corr_tb <- RT_corr_tb[!duplicated(RT_corr_tb$value),]

l1 <- RT_corr_tb$var1
l2 <- RT_corr_tb$var2

# Create table with high correlated predictors
RT_corr_tb[sample(1:nrow(RT_corr_tb), size = nrow(RT_corr_tb)),] %>% 
  datatable(filter = 'top', options = list(
    pageLength = 10, autoWidth = T
  ))

```

**I will not be removing ALL the columns identified as highly correlated since PCA already takes that into account. However, I will remove some highly correlated ones as it makes the clustering analysis easier to interpret**

### Scatter Plots (Highly Correlated Variables)

Lets see the scatter plot of all `r length(l1)` numeric columns in the data

**A few things to note from the correlation analysis**

1) There are 9 features with a correlation above 0.7 between each other.
2) Voucher Plasma, CRP, CPP show high correlation with vessel weight.

```{r Correlation Feat 1, fig.height = 14, fig.width = 10, echo=FALSE}

# Plot predictor correlation (high correlated)
doPlotsCorr(RT_Predictors, plotCorr, 
            l1, l2, 1:9)

```

### Visualizing PCA

```{r, echo=FALSE}
RT_Pred_df3 <- as.data.frame(RT_Predictors)

# Scale data
RT_Pred_df3_scaled <- scale(RT_Pred_df3)

regexp <- "[[:digit:]]+"
res.pca <- prcomp(RT_Pred_df3_scaled, center = TRUE)
eig_tb <- cbind(Dimensions = rownames(get_eig(res.pca)), get_eig(res.pca))
ts <- setDT(eig_tb)[cumulative.variance.percent > 90][1,1]
ts <- str_extract(as.character(ts[[1]]), regexp)

paste('No of PCA: ',ts,sep = "")

n <- as.numeric(ts)
col_list <- list()
for (i in 1:n){ 
  col_list[i]<-paste('rotation.PC',i, sep="") 
} 

pca_df <- as.data.frame(res.pca[2])
pca_df <- pca_df[,colnames(pca_df) %in% col_list]
pca_df <- cbind(Features = rownames(pca_df), pca_df)
pca_df <- setDT(pca_df)[order (rotation.PC1, decreasing = TRUE)]

threshold <- 0.20

pca_df[pca_df < threshold]<-NA

pca_df[sample(1:nrow(pca_df), size = nrow(pca_df)),] %>% 
  datatable(filter = 'top', options = list(
    pageLength = 10, autoWidth = T
  ))

fviz_eig(res.pca)

comp <- data.frame(res.pca$x[,1:4])
```

There are **nine PCs that capture 90% of the variance in the data.**

The scree plot shows that **PC1 captured ~ 32% of the variance**

```{r testgl4, webgl=TRUE, echo=FALSE}

plot_ly(comp, x = ~PC1, y = ~PC2, z = ~PC3, 
        colors = c('green'))

```

**PC1, PC2 and PC3 capture ~60% of the variance in the data.**

**Conclusion**

After meeting with *Manufacturing Science*, the team decided to remove the following features:

1. Cryo_Rich_Plasma   
2. Cryo_Poor_Plasma  


```{r correlation removal, echo=FALSE}

Source_Bottles_New <- Source_Bottles_Raw %>%
  mutate(Delta_Cryo_Cent_W = Vessel_Weight_After_Saline - FrI_Vessel_Weight)

# Remove features that have been transformed
Source_Bottles_New <- Source_Bottles_New[,-c(4, 13, 14, 15, 17, 18)]

# Reorder features
Source_Bottles_New <- Source_Bottles_New[,c(1:14,16,17,19,15,18)]

```
  
  
Additionally, the following predictors have been feature engineering and applied to dataset:
  
  
**1. Actual Saline in Vessel = Delta between Cryo Vessel Weight before and after Saline flush**  
  
**2. Plasma loss Bin Process = Delta between Voucher Plasma and Cryo Vessel Weight before Saline**   
  
**3. Delta Cryo Cent Weight = Delta between Vessel Weight After Saline and Fr I Vessel Weight**
  
**4. Pooling Results = Good Pooling if Pooling Loss <= 1.7%, Bad Pooling if Pooling >1.7%**  
  
  
# Source Bottle Pooling Loss - Modeling

## Outlier Analysis {.tabset .tabset-fade .tabset-pills}

After evaluating dataset for missing data, I proceed to remove possible outliers.

For this step, I proceed to perform the following algorithms to our dataset: *Mahalanobis, Leverage & Cook's Distance*
  
Data that shows on at least 2 of the 3 algorithms will be removed.

```{r Outlier Removal}

# We need to clean the data to make sure no outliers interfere with model

## Multivariable Regression Model is created to evaluate outliers
Source_Bottle_Base_Model <- lm(Pooling_Loss ~ ., Source_Bottles_New[,-c(1:3,19)])

##mahalanobis
MahaSourceBottle <- mahalanobis(Source_Bottles_New[,-c(1:3,19)], 
                        colMeans(Source_Bottles_New[,-c(1:3,19)], na.rm = TRUE),
                        cov(Source_Bottles_New[,-c(1:3,19)], use = "pairwise.complete.obs"))

# Create a cutoff value based on z value
cutoffSourceBottle <- qchisq(1-.001, ncol(Source_Bottles_New[,-c(1:3,19)]))
# Select variables that will be highlighted as outlier
badmahalSourceBottle <- as.numeric(MahaSourceBottle > cutoffSourceBottle)

##leverage
ksb <- ncol(Source_Bottles_New[,-c(1:3,19)]) - 1 ##number of IVs
leveragesourcebottle <- hatvalues(Source_Bottle_Base_Model)
# Create a cutoff value based on leverage
cutleveragesourcebottle <- (2*ksb+2) / nrow(Source_Bottles_New[,-c(1:3,19)])
# Select variables that will be highlighted as outlier
badleveragesourcebottle <- as.numeric(leveragesourcebottle > cutleveragesourcebottle)

##cooks distance
cookssourcebottle <- cooks.distance(Source_Bottle_Base_Model)
# Create cutoff value
cutcookssourcebottle <- 4 / (nrow(Source_Bottles_New[,-c(1:3,19)]) - ksb - 1)
# Select variable that will be highlighted as outlier
badcookssourcebottle <- as.numeric(cookssourcebottle > cutcookssourcebottle)


##overall outliers
##add outliers from each algorithm
totaloutsb <- badmahalSourceBottle + badleveragesourcebottle + badcookssourcebottle

##get rid of set of outliers that have been identified by at least 2 algorithms
Source_Bottles_No_Out <- subset(Source_Bottles_New, totaloutsb < 2)

# Re-ordering features and remove time in 0 minutes
Source_Bottles_No_Out <- Source_Bottles_No_Out %>%
  filter(Wait_Saline_Start>0)

```
  
After applying outlier algorithms to our *Rollerthaw dataset*, I've removed `r nrow(Source_Bottles_New) - nrow(Source_Bottles_No_Out)` lots from dataset.
  
We now visualize new data spread:

### Histogram 1

```{r New Histogram 1, dev='png', echo=FALSE}

doPlots(Source_Bottles_No_Out[,-c(1:3, 19)], plotHist, ii = 1:8)

```

### Histogram 2

```{r New Histogram 2, dev='png', echo=FALSE}

doPlots(Source_Bottles_No_Out[,-c(1:3, 19)], plotHist, ii = 9:15)

```
  
As we can observe from histograms, some features are left and right skewed.  
  
## Feature Selection & Reduction {.tabset .tabset-fade .tabset-pills}

On this step, I proceed to use different algorithms to find the best subset towards explaining output variable.

I first proceed to scale dataset (log10) to normalize skewed features and output variable and then create a correlation plot.

```{r corr plot norm, echo=FALSE,dev='png'}

# Create a new variable from Source Bottle no outlier and normalize it
RT_Source_Norm <- log10(Source_Bottles_No_Out[,-c(1:3,19)])

RT_Source_Norm$Pooling_Result <- Source_Bottles_No_Out$Pooling_Result

# RT_Source_Norm %>%
#   GGally::ggpairs(columns = 1:15,
#                   title = "Correlation Plot (log10) by Pooling Result",
#                   ggplot2::aes(colour=Pooling_Result))

library(psych)
### Check for non-linearity (visually) and transform variables
pairs.panels(RT_Source_Norm[,-c(16)],
             gap = 0,
             bg = c("red", "blue")[RT_Source_Norm$Pooling_Result],
             pch=21,
             main="Correlation Plot (log10) by Pooling Result",
             hist.col="green")


```

Now I proceed to feature engineer categorical variables and divide dataset into training/testing sets:

```{r divide dataset}

RT_Source_Norm$Cryo_Vessel <- Source_Bottles_No_Out$Cryo_Vessel

Pooling_Result <- RT_Source_Norm$Pooling_Result

# Use Dummies to get dummy variables from categorical data:
dummyframeSB <- dummyVars(" ~ .", RT_Source_Norm[,-c(16)])
RT_Source_Norm <- data.frame(predict(dummyframeSB, RT_Source_Norm))

RT_Source_Norm$Pooling_Result <- Pooling_Result

RT_Source_Norm <- RT_Source_Norm[,c(1:14, 16:17, 15, 18)]

## we break the data into training and testing sets
set.seed(100)
# sample <- sample(c(TRUE, FALSE), nrow(RT_Source_Norm), 
#                  replace = T, prob = c(0.7,0.3))
# training_source_bottle <- RT_Source_Norm[sample, ]
# testing_source_bottle <- RT_Source_Norm[!sample, ]

# save(training_source_bottle, file = "rda/training_source_bottle.rda")
# save(testing_source_bottle, file = "rda/testing_source_bottle.rda")

# Load saved datasets
load(file = "rda/training_source_bottle.rda")
load(file = "rda/testing_source_bottle.rda")

```
  
Training dataset has `r nrow(training_source_bottle)` and testing dataset has `r nrow(testing_source_bottle)`
  
**Now I proceed to perform *feature selection* on training dataset:**

```{r, echo=FALSE}
# Using feature selection to remove extra non-useful features that is not 
# improving the model
library(doSNOW) # enable package for multi-core training
library(parallel)

# detect cores with parallel() package
nCores <- detectCores(logical = FALSE)
cat(nCores, " cores detected.")

# detect threads with parallel()
nThreads<- detectCores(logical = TRUE)
cat(nThreads, " threads detected.")

cluster = makeCluster(nThreads, type = "SOCK")

# register the cluster
registerDoSNOW(cluster)
```


### Random Forest (Classification)

```{r RF Class Feature Sel, echo = FALSE, dev='png'}

# We first understand feature importance on training data for 600 runs
# by running random forest
# b_source_bottles <- Boruta(Pooling_Result ~., training_source_bottle[,-c(17)], 
#                            doTrace = 0, maxRuns = 800)

# Save RF Classification feature selection
# save(b_source_bottles, file = "rda/b_source_bottles.rda")

## Load model
load("rda/b_source_bottles.rda")

print(b_source_bottles)

#TentativeRoughFix(b_source_bottles)
par(mfrow = c(1,1))
plot(b_source_bottles, las=2, cex.axis = 0.5, main = "Importance Plot - RF Classification")

plotImpHistory(b_source_bottles, main="Classifier Plot - RF Classification")

# Run stats on model
attStats(b_source_bottles)

paste("The best predictors are: ")

# Get the important features
getSelectedAttributes(b_source_bottles)
```

### Random Forest (Regression)

```{r RF Reg Feature Sel, echo = FALSE, dev='png'}

# We first understand feature importance on training data for 600 runs
# by running random forest
# b_source_bottles_r <- Boruta(Pooling_Loss ~., training_source_bottle[,-c(18)], 
#                            doTrace = 0, maxRuns = 800)

# Save RF Regression feature selection
# save(b_source_bottles_r, file = "rda/b_source_bottles_r.rda")

## Load model
load("rda/b_source_bottles_r.rda")

print(b_source_bottles_r)

#TentativeRoughFix(b_source_bottles_r)
par(mfrow = c(1,1))
plot(b_source_bottles_r, las=2, cex.axis = 0.5, main = "Importance Plot - RF Regression")

plotImpHistory(b_source_bottles_r, main="Classifier Plot - RF Regression")

# Run stats on model
attStats(b_source_bottles_r)

paste("The best predictors are: ")

getSelectedAttributes(b_source_bottles_r)
```

### Best Subset (Regression)

```{r Best subset, echo=FALSE, dev='png'}

# perform best subset selection using feature importance
# best_subset <- regsubsets(Pooling_Loss ~ ., training_source_bottle[,-c(18)], 
#                           nvmax = 10)

# Save Regression Best Subset feature selection
# save(best_subset, file = "rda/best_subset.rda")

## Load model
load("rda/best_subset.rda")

results <- summary(best_subset)
#summary(best_subset)

plot(results$cp, xlab="Number of Features", 
     ylab="Cp Statistics", 
     main = "# Fetures by Cp")

plot(best_subset, scale="Cp", 
     main="Feature Interaction with Model")

# extract and plot results
tibble(predictors = 1:10,
       adj_R2 = results$adjr2,
       Cp = results$cp,
       BIC = results$bic) %>%
  tidyr::gather(statistic, value, -predictors) %>%
  ggplot(aes(predictors, value, color = statistic)) +
  geom_line(show.legend = F) +
  geom_point(show.legend = F) +
  facet_wrap(~ statistic, scales = "free") +
  ggtitle("Best Feature Subset Plot")
  

# Find what subset we should use for our predictors
cat("Feature selection - Best #features by RAdj^2:" , which.max(results$adjr2))
#which.max(results$adjr2)
coef(best_subset, which.max(results$adjr2))

cat("Feature selection - Best #features by bic:" , which.min(results$bic))
#which.min(results$bic)
coef(best_subset, which.min(results$bic))

cat("Feature selection - Best #features by cp:" , which.min(results$cp))
#which.min(results$cp)
coef(best_subset, which.min(results$cp))

# saving the best subset of features
best_subset_by_RAdj <- coef(best_subset, which.max(results$adjr2))

########### Estimating the test error for each subset of predictors ##############

test_SouceBottle <- model.matrix(Pooling_Loss ~ ., training_source_bottle[,-c(20)])

# Create a loop for each model to compute the test MSE (mean squared error)

# create empty vector to fill with error values
validation_errors <- vector("double", length = 10)

for(i in 1:which.max(results$adjr2)) {
  coef_x <- coef(best_subset, id = i)                     # extract coefficients for model size i
  pred_x <- test_SouceBottle[ , names(coef_x)] %*% coef_x           # predict pooling loss using matrix algebra
  validation_errors[i] <- mean((training_source_bottle$Pooling_Loss - pred_x)^2)  # compute test error btwn actual & predicted pooling loss
}

# plot validation errors
plot(validation_errors, type = "b", main= "MSE for Each Feature Subset")

```

### Gradient Boosting (Regression)

```{r Gradient Boosting, echo=FALSE, dev='png'}
# Feature selection using gradient boosting
# SourceBottleBestFeatures <- featureSelection(data = training_source_bottle[,-c(18)],
#                                            target = "Pooling_Loss",
#                                            index = NULL,
#                                            selection = selectionControl(n_rounds = 100,
#                                                                         n_mods = 1100,
#                                                                         p = 30,
#                                                                         penalty = 0.3,
#                                                                         reward = 0.2),
#                                            bootstrap = "regular",
#                                            boosting = boostingControl(mstop = 100,
#                                                                       nu = 0.1),
#                                            early_stopping = "aic",
#                                            cores = nCores)

# Save Gradient Boosting feature selection object to load later
#save(SourceBottleBestFeatures, file = "rda/SourceBottleBestFeatures.rda")
load(file = "rda/SourceBottleBestFeatures.rda")

print(SourceBottleBestFeatures)

plot(SourceBottleBestFeatures, n_features = 8)

```

### Recursive Feature Elimination (Classification)

```{r RFE, echo=FALSE}

#Feature selection using rfe in caret
control <- rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   repeats = 10,
                   verbose = FALSE)

# Feature_RT_Profile <- rfe(training_source_bottle[,-c(17,18)], 
#                          training_source_bottle[,c(18)],
#                          rfeControl = control)

# Save RFE Class feature selection
# save(Feature_RT_Profile, file = "rda/Feature_RT_Profile.rda")

## Load model
load("rda/Feature_RT_Profile.rda")

Feature_RT_Profile

RT_Predictors_Best_RFE <- predictors(Feature_RT_Profile)

plot(Feature_RT_Profile, type = c("g", "o"),
     main = "Recursive Feature Elimination - Classification")

plot(Feature_RT_Profile, type = c("g", "o"), metric = "Kappa",
     main = "Recursive Feature Elimination - Classification")

paste("The best predictors are: ")
RT_Predictors_Best_RFE

```

### Recursive Feature Elimination (Regression)

```{r RFE Regression, echo=FALSE}

#Feature selection using rfe in caret
control <- rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   repeats = 10,
                   verbose = FALSE)

# Feature_RT_Profile_R <- rfe(training_source_bottle[,-c(17,18)], 
#                          training_source_bottle[,c(17)],
#                          rfeControl = control)

# Save RFE Reg feature selection
# save(Feature_RT_Profile_R, file = "rda/Feature_RT_Profile_R.rda")

## Load model
load("rda/Feature_RT_Profile_R.rda")


Feature_RT_Profile_R

RT_Predictors_Best_RFE_R <- predictors(Feature_RT_Profile_R)

plot(Feature_RT_Profile_R, type = c("g", "o"),
     main = "Recursive Feature Elimination - Regression")

# The resampling profile can be visualized along with plots of the individual resampling results:
#1
plot(Feature_RT_Profile_R, type = c("g", "o"), metric = "Rsquared",
     main = "Recursive Feature Elimination - Regression")

paste("The best predictors are: ")
RT_Predictors_Best_RFE_R

```

## Modeling {.tabset .tabset-fade .tabset-pills}

**For this step, I apply different machine learning models to explain output variable.**

*Stratification and Cross-validation* will be used on training dataset.

### Random Forest (Classification)

```{r RF 1, echo=FALSE, dev='png' }

##################### Random Forest K cross validation - Model 1 ##################################

# Leverage caret to create 10x10 total folds but I need to ensure that the ratio of pooling loss
# above and below target on each fold matches the overall training set. This
# is known as stratified cross validation and generally provides better results.

# In the first step, multifold models will be generated 
set.seed(2344)
cv.10.folds <- createMultiFolds(training_source_bottle$Pooling_Result, 
                                k = 5, times = 20)

# Now lets check the startification
#table(training_source_bottle$Pooling_Result)

#table(training_source_bottle$Pooling_Result[cv.10.folds[[17]]])

# Set up caret's trainControl object per above.
set.seed(119)
control <- trainControl(method="repeatedcv", number = 5, repeats=10,
                        index = cv.10.folds, search = "grid",
                        classProbs = TRUE,
                        summaryFunction = twoClassSummary)

# Parameter grid
parameterGrid <- expand.grid(mtry=c(2:16))

# Train the model
# fitSourceBottle.rf.log <- train(Pooling_Result~., training_source_bottle[,-c(17)],
#                                 method="rf", trControl=control, ntree=2000,
#                                 tuneGrid = parameterGrid,
#                                 keep.inbag = T, localImp = TRUE,
#                                  metric = "ROC")

# Save RF Class Model
# save(fitSourceBottle.rf.log, file = "rda/fitSourceBottle.rf.log.rda")

## Load model
load("rda/fitSourceBottle.rf.log.rda")

#print(fitSourceBottle.rf.log)
print(fitSourceBottle.rf.log$finalModel)

plot(fitSourceBottle.rf.log, main = "ROC VS Features")

#fitSourceBottle.rf.log$results
SourceBottle_RF_Model1 <- fitSourceBottle.rf.log$finalModel

##Finally, we can see the features that explain our forest model:
# varImpPlot(SourceBottle_RF_Model1,sort = T, n.var = 8, 
#            main = "Top 8 features for Pooling Loss - Random Forest Model 1 (Classification)")

# Plot Feature Importance
plot(varImp(object=fitSourceBottle.rf.log, scale = FALSE),
     main="Feature Importance - Random Forest Model 1 (Classification)")

# # Plot min depht distribution of tree
# SourceBottle_min_depth_model1 <- min_depth_distribution(SourceBottle_RF_Model1)
# plot_min_depth_distribution(SourceBottle_min_depth_model1, 
#                             mean_sample = "relevant_trees", 
#                             k = 10)

#Show importance plot for model
SourceBottle_importance_frame_model1 <- measure_importance(SourceBottle_RF_Model1)

# Drop NAs from importance dataset
SourceBottle_importance_frame_model1 <- SourceBottle_importance_frame_model1 %>%
  drop_na()

plot_multi_way_importance(SourceBottle_importance_frame_model1, size_measure = "p_value",
                          x_measure = "accuracy_decrease", 
                          y_measure = "gini_decrease", 
                          no_of_labels = nrow(SourceBottle_importance_frame_model1),
                          main = "Top features for Pooling Loss - Random Forest Model 1")

print("Top 8 Important Features:")
important_variables(SourceBottle_importance_frame_model1, 
                    k = 8)

# Create a variable from important features from forest based on min depth and trees
VarsModel1 <- important_variables(SourceBottle_importance_frame_model1, 
                            k = nrow(SourceBottle_importance_frame_model1), 
                            measures = c("mean_min_depth", "no_of_trees"))

# Create interactions between important features by min depth
interactions_frame_model1 <- min_depth_interactions(SourceBottle_RF_Model1,
                                               VarsModel1, 
                                               mean_sample = "relevant_trees", 
                                               uncond_mean_sample = "relevant_trees")

print("Report Top Feature Iteractions (order by occurrences): ")
# Print interactions
knitr::kable(head(interactions_frame_model1[order(interactions_frame_model1$occurrences, 
                                     decreasing = TRUE), ]))

# Plot Interactions
plot_min_depth_interactions(interactions_frame_model1)

print("Report best features: ")
c(lapply(SourceBottle_importance_frame_model1[2], function(x) {
      setNames(sort(x, decreasing=F), 
               SourceBottle_importance_frame_model1$variable[order(x, decreasing=F)])
    }),
    lapply(SourceBottle_importance_frame_model1[-(1:2)], function(x) {
      setNames(sort(x, decreasing=T), 
               SourceBottle_importance_frame_model1$variable[order(x, decreasing=T)])
  }))

library(forestFloor)
#compute forestFloor object, often only 5-10% time of growing forest
SourceBottle_forestf1 <- forestFloor(
  rf.fit = SourceBottle_RF_Model1,      
  X = training_source_bottle[,-c(17,18)],            
  Y = training_source_bottle[,c(18)],
  calc_np = FALSE,    # TRUE or FALSE both works, makes no difference
  binary_reg = TRUE  
)

print("Partial Feature Contribution:")

#plot partial functions of most important variables first
plot(SourceBottle_forestf1,                       # forestFloor object
     plot_seq = 1:9,           # optional sequence of features to plot
     orderByImportance=TRUE,    # if TRUE index sequence by importance, else by X column  
     col=fcol(SourceBottle_forestf1)
)

# probabilites 
library(pROC)
Pred_Forest_M1 <- predict(fitSourceBottle.rf.log, 
                          training_source_bottle[,-c(17)], type='prob')

# Calculate AUC
auc <- roc(ifelse(training_source_bottle[,c(18)]=="Good_PL",1,0), 
           Pred_Forest_M1[[2]])

RF_ConfM_Log_Bottle_Model1 <- confusionMatrix(training_source_bottle$Pooling_Result,
                                      predict(SourceBottle_RF_Model1,training_source_bottle[,-c(17)]))

#the accuracy of model on test dataset
Model1_Accuracy <- (sum(predict(fitSourceBottle.rf.log,training_source_bottle[,-c(17)])==
                            training_source_bottle$Pooling_Result)
                      /length(training_source_bottle$Pooling_Result))

paste("Random Forest (Model 1) Stats:")

# Validation Accuracy
RF_ConfM_Log_Bottle_Model1$overall

print(auc$auc)


```

### Random Forest (Regression)

```{r RF 2, echo=FALSE, dev='png' }

##################### Random Forest K cross validation - Model 2 ##################################


# Set up caret's trainControl object per above.
set.seed(111)
control <- trainControl(method="repeatedcv", number = 5, repeats=10,
                        search = "grid")

# Parameter grid
parameterGrid <- expand.grid(mtry=c(2:16))

# Train the model
# fitSourceBottle.rf2 <- train(Pooling_Loss~., training_source_bottle[,-c(18)], 
#                              method="rf", trControl=control, ntree=2000, 
#                              tuneGrid = parameterGrid, keep.inbag = T, localImp = TRUE)

# Save RF Reg Model
# save(fitSourceBottle.rf2, file = "rda/fitSourceBottle.rf2.rda")

## Load model
load("rda/fitSourceBottle.rf2.rda")

fitSourceBottle.rf2$finalModel

plot(fitSourceBottle.rf2, main = "RMSE VS Features")

#fitSourceBottle.rf2$results
SourceBottle_RF_Model2 <- fitSourceBottle.rf2$finalModel

##Finally, we can see the features that explain our forest model:
# varImpPlot(SourceBottle_RF_Model2,sort = T, n.var = 8,
#            main = "Top 8 features for Pooling Loss - Random Forest Model 2 (Regression)")


plot(varImp(object=fitSourceBottle.rf2, scale = FALSE),
     main="Feature Importance - Random Forest Model 2 (Regression)")

# # Plot min depht distribution of tree
# SourceBottle_min_depth_model2 <- min_depth_distribution(SourceBottle_RF_Model2)
# plot_min_depth_distribution(SourceBottle_min_depth_model2, 
#                             mean_sample = "relevant_trees", 
#                             k = 10)

#Show importance plot for model
SourceBottle_importance_frame_model2 <- measure_importance(SourceBottle_RF_Model2)
plot_multi_way_importance(SourceBottle_importance_frame_model2, size_measure = "p_value",
                          x_measure = "mse_increase", 
                          y_measure = "node_purity_increase", 
                          no_of_labels = nrow(SourceBottle_importance_frame_model2),
                          main = "Top features for Pooling Loss - Random Forest Model 2")


print("Top 8 Important Features:")
important_variables(SourceBottle_importance_frame_model2, 
                    k = 8)


# Create a variable from important features from forest based on min depth and trees
VarsModel2 <- important_variables(SourceBottle_importance_frame_model2, 
                            k = nrow(SourceBottle_importance_frame_model2), 
                            measures = c("mse_increase", "node_purity_increase"))

# Create interactions between important features by min depth
interactions_frame_model2 <- min_depth_interactions(SourceBottle_RF_Model2,
                                               VarsModel2, 
                                               mean_sample = "relevant_trees", 
                                               uncond_mean_sample = "relevant_trees")

print("Report Top Feature Iteractions (order by occurrences): ")
# Print interactions
knitr::kable(head(interactions_frame_model2[order(interactions_frame_model2$occurrences, 
                                     decreasing = TRUE), ]))

# Plot Interactions
plot_min_depth_interactions(interactions_frame_model2)

print("Report best features: ")
c(lapply(SourceBottle_importance_frame_model2[2], function(x) {
      setNames(sort(x, decreasing=F), 
               SourceBottle_importance_frame_model2$variable[order(x, decreasing=F)])
    }),
    lapply(SourceBottle_importance_frame_model2[-(1:2)], function(x) {
      setNames(sort(x, decreasing=T), 
               SourceBottle_importance_frame_model2$variable[order(x, decreasing=T)])
  }))


#compute forestFloor object, often only 5-10% time of growing forest
SourceBottle_forestf2 <- forestFloor(
  rf.fit = SourceBottle_RF_Model2,      
  X = training_source_bottle[,-c(17,18)],            
  Y = training_source_bottle[,c(17)],
  calc_np = FALSE,    # TRUE or FALSE both works, makes no difference
  binary_reg = FALSE  
)

print("Partial Feature Contribution:")

#plot partial functions of most important variables first
plot(SourceBottle_forestf2,                       # forestFloor object
     plot_seq = 1:9,           # optional sequence of features to plot
     orderByImportance=TRUE,    # if TRUE index sequence by importance, else by X column  
     col=fcol(SourceBottle_forestf2)
)


# Check how good is the model on the training set - correlation^2, RME and MAE
train.corr.rf2 <- round(cor(10^predict(SourceBottle_RF_Model2, 
                                          training_source_bottle[,-c(18)]), 
                        10^training_source_bottle$Pooling_Loss), 2)

train.RMSE.rf2 <- round(sqrt(mean((10^ predict(SourceBottle_RF_Model2, 
                                          training_source_bottle[,-c(18)]) - 
                                 10^ training_source_bottle$Pooling_Loss)^2)))

train.MAE.rf2 <- round(mean(abs(10^predict(SourceBottle_RF_Model2, 
                                          training_source_bottle[,-c(18)]) - 
                              10^ training_source_bottle$Pooling_Loss)))

## plot to compare test and validation data
plot(10^training_source_bottle$Pooling_Loss, type="l", lty=1.8, col="red", 
     main = "Pooling Loss - Actuals VS Predicted (Random Forest Model 2)",
     xlab = "Lots", 
     ylab = "Pooling Loss (%)")
lines(10^ predict(SourceBottle_RF_Model2, 
                  training_source_bottle[,-c(18)]), 
      type = "l", lty = 1.8, col="blue")


paste("Random Forest (Model 2) Stats: (R^2, RMSE and MAE)")
c(train.corr.rf2^2, train.RMSE.rf2, train.MAE.rf2)

```


### Gradient Boosting Machine (Classification)

```{r GBM, echo=FALSE, dev='png'}

# In the first step, multifold models will be generated 
set.seed(2300)
cv.10.folds <- createMultiFolds(training_source_bottle$Pooling_Result, 
                                k = 5, times = 20)

# Now lets check the startification
#table(training_source_bottle$Pooling_Result)

#table(training_source_bottle$Pooling_Result[cv.10.folds[[17]]])

# Set up caret's trainControl object per above.
set.seed(115)
control <- trainControl(method="repeatedcv", number = 5, repeats=10,
                        index = cv.10.folds,
                        classProbs = TRUE,
                        summaryFunction = twoClassSummary)

# Parameter grid
parameterGrid <-  expand.grid(n.trees = (0:20)*50,
                              shrinkage=c(0.01,0.05,0.1,0.5),
                              n.minobsinnode = c(1,3,5),
                              interaction.depth=c(1,5,10))
                        
set.seed(800)
# fitSourceBottle.gbm3.tune <- train(x = training_source_bottle[,-c(17,18)],
#                               y = training_source_bottle[,c(18)],
#                               method = "gbm",
#                               trControl = control,
#                               tuneGrid = parameterGrid,
#                               verbose = FALSE,
#                               metric = "ROC")

# Save Model
# save(fitSourceBottle.gbm3.tune, file = "rda/fitSourceBottle.gbm3.tune.rda")

## Load model
load("rda/fitSourceBottle.gbm3.tune.rda")

#Best tune
print("Based tunning parameters based on initial GBM model:")
fitSourceBottle.gbm3.tune$bestTune

plot(fitSourceBottle.gbm3.tune,
     main="Performance Profile - ROC")

plot(fitSourceBottle.gbm3.tune, metric = "Sens", plotType = "level",
     scales = list(x = list(rot = 90)))

plot(fitSourceBottle.gbm3.tune, metric = "Spec", plotType = "level",
     scales = list(x = list(rot = 90)))

# Now we run the model using the best tuning as grid
parameterGrid <- expand.grid(fitSourceBottle.gbm3.tune$bestTune)

set.seed(805)
# fitSourceBottle.gbm3 <- train(x = training_source_bottle[,-c(17,18)],
#                               y = training_source_bottle[,c(18)],
#                               method = "gbm",
#                               trControl = control,
#                               tuneGrid = parameterGrid,
#                               verbose = FALSE,
#                               metric = "ROC")

# Save Model
# save(fitSourceBottle.gbm3, file = "rda/fitSourceBottle.gbm3.rda")

## Load model
load("rda/fitSourceBottle.gbm3.rda")

print("Final Model based on tunning parameters above:")
fitSourceBottle.gbm3

#Final Model
SourceBottle_GBM_Model3 <- fitSourceBottle.gbm3$finalModel

plot(varImp(object=fitSourceBottle.gbm3, scale = FALSE),
     main="Feature Importance - Gradient Boosting Machine Model 3 (Classification)")

varImp(object=fitSourceBottle.gbm3)

# par(mar = c(5, 11, 1, 1))
# summary(
#   fitSourceBottle.gbm3,
#   cBars = 10,
#   method = permutation.test.gbm, # also can use permutation.test.gbm
#   las = 2,
#   main = "Feature Importance by Average Accuracy Decrease"
#   )

fitSourceBottle.gbm3 %>%
  partial(pred.var = "Plasma_loss_Bin_Process", 
          n.trees = fitSourceBottle.gbm3$finalModel$n.trees, 
          grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = training_source_bottle[,-c(17)],
           main = "Partial Dependance Plot on Main Feature")

# Two Variables
par.Recipe.Saline <- partial(fitSourceBottle.gbm3, 
                             pred.var = c("Recipe_Load", "Actual_Saline_in_Vessel"), 
                             chull = TRUE)

# par.Recipe.Saline$Recipe_Load <- 10^par.Recipe.Saline$Recipe_Load
# par.Recipe.Saline$Saline_Weight <- 10^par.Recipe.Saline$Saline_Weight
# par.Recipe.Saline$yhat <- 10^par.Recipe.Saline$yhat

autoplot(par.Recipe.Saline, contour = TRUE, 
         legend.title = "Partial\ndependence",
         main = "Partial Dependency Plot")

# #Predictions
Pred_GBM <- predict(fitSourceBottle.gbm3, training_source_bottle[,-c(17)],
                        type="raw",
                        n.trees = fitSourceBottle.gbm3$finalModel$n.trees)

# probabilites 
Pred_GBM_Model3 <- predict(fitSourceBottle.gbm3, 
                           training_source_bottle[,-c(17)], type='prob')

# Calculate AUC
auc_M3 <- roc(ifelse(training_source_bottle[,c(18)]=="Good_PL",1,0), 
              Pred_GBM_Model3[[2]])

RF_ConfM_Log_Bottle_Model3 <- confusionMatrix(training_source_bottle$Pooling_Result,
                                              Pred_GBM)

#the accuracy of model on test dataset
Model3_Accuracy <- (sum(Pred_GBM == training_source_bottle$Pooling_Result)
                      /length(training_source_bottle$Pooling_Result))

paste("Gradient Booting Machine (Model 3) Stats:")

# Validation Accuracy
RF_ConfM_Log_Bottle_Model3$overall

auc_M3$auc

```

### k-Nearest Neighbors (Classification)

```{r KNN , echo=FALSE, dev='png'}
# We create new training/testing sets and feature engineer response variable

##################### KNN K cross validation Using ROC - Model 7  ##################################

# Leverage caret to create 10x10 total folds but I need to ensure that the ratio of pooling loss
# above and below target on each fold matches the overall training set. This
# is known as stratified cross validation and generally provides better results.

# Set up caret's trainControl object per above.
set.seed(195)
cv.10.folds <- createMultiFolds(training_source_bottle$Pooling_Result, 
                                k = 5, times = 20)

# Set up caret's trainControl object per above.
set.seed(1155)
control <- trainControl(method="repeatedcv", number = 5, repeats=10,
                        index = cv.10.folds,
                        classProbs = TRUE,
                        summaryFunction = twoClassSummary)

set.seed(77)
# Train the model with PreProcess center and scaling
# fitSourceBottle.log.knn.tune <- train(Pooling_Result~., training_source_bottle[,-c(17)],
#                                       method="knn",
#                                       trControl=control,
#                                       tuneGrid = expand.grid(k=1:50),
#                                       metric = "ROC")

# Save Model
# save(fitSourceBottle.log.knn.tune, file = "rda/fitSourceBottle.log.knn.tune.rda")

## Load model
load("rda/fitSourceBottle.log.knn.tune.rda")

#Best tune
print("Based tunning parameters based on initial GBM model:")
fitSourceBottle.log.knn.tune$bestTune

plot(fitSourceBottle.log.knn.tune,
     main = "Best # Neighbors")

set.seed(8051)
# fitSourceBottle.log.knn <- train(Pooling_Result~., training_source_bottle[,-c(17)],
#                                  method = "knn",
#                                  trControl = control,
#                                  tuneGrid = fitSourceBottle.log.knn.tune$bestTune,
#                                  metric = "ROC")

# Save Model
# save(fitSourceBottle.log.knn, file = "rda/fitSourceBottle.log.knn.rda")

## Load model
load("rda/fitSourceBottle.log.knn.rda")

print("Final Model based on tunning parameters above:")
fitSourceBottle.log.knn

SourceBottle_KNN_Model4 <- fitSourceBottle.log.knn$finalModel

plot(varImp(object=fitSourceBottle.log.knn),
     main="Feature Importance - K-Nearest Neighbor Model 4 (Classification)")

varImp(object=fitSourceBottle.log.knn)

#Create Predictions based on model
Pred_KNN <- predict(fitSourceBottle.log.knn, 
                    training_source_bottle[,-c(17)],
                    type = "raw")

Pred_KNN_Model4 <-  predict(fitSourceBottle.log.knn, 
                           training_source_bottle[,-c(17)], 
                           type='prob')

# Calculate AUC
auc_M4 <- roc(ifelse(training_source_bottle[,c(18)]=="Good_PL",1,0), 
              Pred_KNN_Model4[[2]])

#Show Confusion Matrix
KNN_Confusion_Matrix_M4 <-confusionMatrix(training_source_bottle$Pooling_Result,
                                          Pred_KNN)
fitSourceBottle.log.knn %>%
  partial(pred.var = "Plasma_loss_Bin_Process", 
          grid.resolution = 10) %>%
  autoplot(rug = TRUE, train = training_source_bottle[,-c(17)],
           main = "Partial Dependance Plot on Main Feature") 

# Two Variables
par.Recipe.Saline <- partial(fitSourceBottle.log.knn, 
                             pred.var = c("Actual_Saline_in_Vessel", 
                                          "Bin_Performance"), 
                             chull = TRUE)

# par.Recipe.Saline$Recipe_Load <- 10^par.Recipe.Saline$Recipe_Load
# par.Recipe.Saline$Saline_Weight <- 10^par.Recipe.Saline$Saline_Weight
# par.Recipe.Saline$yhat <- 10^par.Recipe.Saline$yhat

autoplot(par.Recipe.Saline, contour = TRUE, 
         legend.title = "Partial\ndependence",
         main = "Partial Dependency Plot")


#the accuracy of model on test dataset
Model4_Accuracy <- (sum(Pred_KNN == training_source_bottle$Pooling_Result)
                      /length(training_source_bottle$Pooling_Result))

paste("k-Nearest Neighbors (Model 4) Stats:")

# Validation Accuracy
KNN_Confusion_Matrix_M4$overall

auc_M4$auc

```

## Compare ML Models (Resampling)

```{r Compare Models, echo=FALSE, dev='png'}

resamps <- resamples(list(RF = fitSourceBottle.rf.log,
                          GBM = fitSourceBottle.gbm3,
                          KNN = fitSourceBottle.log.knn))

summary(resamps)

theme1 <- trellis.par.get()
theme1$plot.symbol$col = rgb(.2, .2, .2, .4)
theme1$plot.symbol$pch = 16
theme1$plot.line$col = rgb(1, 0, 0, .7)
theme1$plot.line$lwd <- 2
trellis.par.set(theme1)
bwplot(resamps, layout = c(3, 1))


trellis.par.set(caretTheme())
dotplot(resamps, metric = "ROC")

# Compare resample distributions

## Making inference between models since we I'm using same training dataset
## to help reduce within-resample correlation that may exist
## We can compute the differences, then use a simple t-test to evaluate the null hypothesis that there is no difference between models.

print("Since I'm using same dataset, I proceed to compute differences between models (t-test):")

difValues <- diff(resamps)
difValues

summary(difValues)

# Boxplot
trellis.par.set(theme1)
bwplot(difValues, layout = c(3, 1))

trellis.par.set(caretTheme())
dotplot(difValues)

```

As we can see from resampling between models, **Gradient Boosting Machine (GBM)** and **Random Forest Classification (RF)** models shows higher *ROC* with an Accuracy of `r Model3_Accuracy` and `r Model1_Accuracy` respectively while **Random Forest Regression (Model 2)** show a R^2 of `r train.corr.rf2` on **training dataset**.

Only **Random Forest Model 1** show high *ROC, Sensitivity and Specificity* values.

## Model on Testing Dataset

We now use **Random Forest Model 2 (Regression)** on *Test Dataset*

```{r Validation dataset, echo= FALSE, dev='png'}

print("Random Forest Regression Model (Model 2)")

SourceBottle_RF_Model2

Pred_Forest <- predict(SourceBottle_RF_Model2, testing_source_bottle[,-c(18)])


# Check how good is the model on the validation set - correlation^2, RME and MAE
validbottle.corr2 <- round(cor(10^Pred_Forest, 
                             10^testing_source_bottle$Pooling_Loss), 2)
validbottle.RMSE2 <- round(sqrt(mean((10^Pred_Forest - 
                                        10^testing_source_bottle$Pooling_Loss)^2)))
validbottle.MAE2 <- round(mean(abs(10^Pred_Forest - 
                                     10^testing_source_bottle$Pooling_Loss)))

# Iteraction frame
plotModel2.dataframe <- data.frame(testing_source_bottle[,-c(18)], 
                                   predicted = Pred_Forest)

plotModel2.dataframe <- 10^plotModel2.dataframe

# First use Convex hull to determine boundary points of each cluster
plotmodel2.df1 = data.frame(x = 10^testing_source_bottle$Plasma_loss_Bin_Process, 
                      y = 10^testing_source_bottle$APDS_Discard, 
                      predicted = 10^Pred_Forest)

# First use Convex hull to determine boundary points of each cluster
plotmodel2.df2 = data.frame(x = 10^testing_source_bottle$Actual_Saline_in_Vessel, 
                      y = 10^testing_source_bottle$Wait_Saline_Start, 
                      predicted = 10^Pred_Forest)

find_hull = function(df) df[chull(df$x, df$y), ]

boundary_model2 = ddply(plotmodel2.df1, .variables = "predicted", .fun = find_hull)
boundary_model2.2 = ddply(plotmodel2.df2, .variables = "predicted", .fun = find_hull)

#Plot Iteraction 1
ggplot(plotModel2.dataframe, aes(Plasma_loss_Bin_Process, 
                                 APDS_Discard,
                                 color = predicted, fill = predicted)) + 
  geom_point(size = 5) + 
  geom_polygon(data = boundary_model2, aes(x,y), alpha = 0.5) + 
  ggtitle("Pooling Loss Source Botte - Random Forest Model 2
          (Plasma Loss During Bin Process VS APDS Discard)")

#Plot Iteraction 2
ggplot(plotModel2.dataframe, aes(Actual_Saline_in_Vessel, 
                                 Wait_Saline_Start,
                                 color = predicted, fill = predicted)) + 
  geom_point(size = 5) + 
  geom_polygon(data = boundary_model2.2, aes(x,y), alpha = 0.5) + 
  ggtitle("Pooling Loss Source Botte - Random Forest Model 2
          (Actual Saline in Vessel VS Wait Saline Start)")


## plot to compare test and validation data
plot(10^testing_source_bottle$Pooling_Loss, type="l", lty=1.8, col="red", 
     main = "Pooling Loss - Actuals VS Predicted (Random Forest Model 2)", xlab = "Lots", 
     ylab = "Pooling Loss (%)")
lines(10^Pred_Forest, type = "l", lty = 1.8, col="blue")

cat("Randomg Forest - Model 2 (RAdj^2 on validation data):" , validbottle.corr2^2)

```

# Source Bottle Pooling Loss - Key Takeaways

1. Based on *Mean Changepoint Analysis* and *Breakpoint Analysis*, Rollerthaw lots show 0 changepoints during the first 2 months of 2020.  
  
2. Forecast analysis show an upper trend for all 2020 with focus on **Monday to Tuesday and Friday to Saturday**.  

```{r StopCluster, echo=FALSE}
# stop cluster and remove clients
stopCluster(cluster); print("Cluster stopped.")

# insert serial backend, otherwise error in repetetive tasks
registerDoSEQ()

# clean up a bit.
invisible(gc); remove(nCores); remove(nThreads); remove(cluster); 


# getModelInfo()$gbm$parameters
# library(parallel)
# library(doMC)
# registerDoMC(cores = 20)
# # Max shrinkage for gbm
# nl = nrow(training)
# max(0.01, 0.1*min(1, nl/10000))
# # Max Value for interaction.depth
# floor(sqrt(NCOL(training)))
# gbmGrid <-  expand.grid(interaction.depth = c(1, 3, 6, 9, 10),
#                     n.trees = (0:50)*50, 
#                     shrinkage = seq(.0005, .05,.0005),
#                     n.minobsinnode = 10) # you can also put something        like c(5, 10, 15, 20)
# 
# fitControl <- trainControl(method = "repeatedcv",
#                        repeats = 5,
#                        preProcOptions = list(thresh = 0.95),
#                        ## Estimate class probabilities
#                        classProbs = TRUE,
#                        ## Evaluate performance using
#                        ## the following function
#                        summaryFunction = twoClassSummary)
# 
# # Method + Date + distribution
# set.seed(1)
# system.time(GBM0604ada <- train(Outcome ~ ., data = training,
#             distribution = "adaboost",
#             method = "gbm", bag.fraction = 0.5,
#             nTrain = round(nrow(training) *.75),
#             trControl = fitControl,
#             verbose = TRUE,
#             tuneGrid = gbmGrid,
#             ## Specify which metric to optimize
#             metric = "ROC"))
```


```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```